like "kitten") with all those billion words. It will check the rate of similarity between this particular word and that word. These models vary in the number of words they carry. A small model has about a billion; a medium-sized model has around 50 billion; a large model has several more. These numbers might be a bit wrong, but that's the distinction. The larger the model, the better the embeddings. Here, we have 1,056. That means it's checking the similarity of a word with 1,056 words in its database. That's how embeddings work. For creating embeddings, there's a library in NLP, actually ML, with a module called word2vec . It's deployed on Hugging Face. Q32: Is Hugging Face part of the TDS course? A32: I think it's been removed. It would be too heavy for TDS. Q33: Can we create LLM applications on Google Colab? A33: Not exactly, but we can make API calls and get embeddings for words. For example, let's say we have the word "Anand".

---

