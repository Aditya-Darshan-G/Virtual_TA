Data Preparation in the Shell You'll learn how to use UNIX tools to process and clean data, covering: curl (or wget ) to fetch data from websites. gzip (or xz ) to compress and decompress files. wc to count lines, words, and characters in text. head and tail to get the start and end of files. cut to extract specific columns from text. uniq to de-duplicate lines. sort to sort lines. grep to filter lines containing specific text. sed to search and replace text. awk for more complex text processing. Data preparation in the shell - Notebook UNIX has a great set of tools to clean and analyze data. This is important because these tools are : Agile : You can quickly explore data and see the results. Fast : They're written in C. They're easily parallelizable. Popular : Most systems and languages support shell commands. In this notebook , we'll explore log files with these shell-based commands.

---

