from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
import os
import numpy as np
import re
import logging
from dotenv import load_dotenv
import asyncio
from google import generativeai as genai

# Load environment variables
load_dotenv()
GEMINI_API_KEY = os.getenv("GENAI_API_KEY")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
EMBEDDING_FILE = os.path.join(os.path.dirname(__file__), "..", "data", "embeddings.npz")
SIMILARITY_THRESHOLD = 0.68
MAX_RESULTS = 10
MAX_CONTEXT_CHUNKS = 4

# Load Gemini
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

# Load embeddings
try:
    data = np.load(EMBEDDING_FILE, allow_pickle=True)
    chunks = data["chunks"]
    embeddings = data["embeddings"]
    source_urls = data["source_urls"]
except Exception as e:
    logger.error("Failed to load embeddings: %s", e)
    raise RuntimeError("Failed to load embeddings from disk")

# App setup
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Schemas
class QueryRequest(BaseModel):
    question: str
    image: Optional[str] = None

class LinkInfo(BaseModel):
    url: str
    text: str

class QueryResponse(BaseModel):
    answer: str
    links: List[LinkInfo]

# Cosine similarity
def cosine_similarity(a, b):
    a, b = np.array(a), np.array(b)
    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
        return 0.0
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# Embed text
async def embed_text(query: str):
    try:
        response = genai.embed_content(
            model="models/embedding-001",
            content=query,
            task_type="semantic_similarity"
        )
        return response["embedding"]
    except Exception as e:
        logger.error("Embedding failed: %s", e)
        raise HTTPException(status_code=500, detail="Embedding failed")

# Search
def search_similar_chunks(query_emb):
    scores = [cosine_similarity(query_emb, e) for e in embeddings]
    sorted_indices = np.argsort(scores)[::-1]
    seen_sources = set()
    results = []

    for idx in sorted_indices:
        if scores[idx] < SIMILARITY_THRESHOLD:
            continue
        url = source_urls[idx]
        if url not in seen_sources:
            seen_sources.add(url)
        results.append({
            "url": url,
            "text": chunks[idx],
            "score": float(scores[idx])
        })
        if len(results) >= MAX_RESULTS:
            break
    return results

# Prompt builder
def build_prompt(question: str, context_snippets: List[dict]) -> str:
    context = "\n\n".join([f"Source: {c['url']}\n{c['text']}" for c in context_snippets])
    return f"""
You are a helpful teaching assistant answering questions from a Data Science course.
Use only the content provided below to answer the question.

Question:
{question}

Sources:
{context}

Respond with:
Answer:
<your answer>

Sources:
1. URL: <url_1>, Text: <quote or explanation>
2. URL: <url_2>, Text: <quote or explanation>
..."""

# Generate answer
async def generate_answer(question: str, context_snippets: List[dict]):
    prompt = build_prompt(question, context_snippets)
    try:
        result = model.generate_content(prompt)
        return result.text.strip()
    except Exception as e:
        logger.error("LLM generation failed: %s", e)
        raise HTTPException(status_code=500, detail="Failed to generate answer")

# Parse output
def extract_answer_and_links(text):
    parts = text.split("Sources:", 1)
    answer = parts[0].strip()
    links = []
    if len(parts) > 1:
        for line in parts[1].strip().split("\n"):
            match = re.search(r'URL:\s*(\S+),\s*Text:\s*(.*)', line)
            if match:
                url, snippet = match.groups()
                links.append({"url": url.strip(), "text": snippet.strip()})
    return {"answer": answer, "links": links}

# API routes
@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    query_emb = await embed_text(request.question)
    top_chunks = search_similar_chunks(query_emb)
    if not top_chunks:
        return QueryResponse(answer="No relevant information found.", links=[])
    raw_output = await generate_answer(request.question, top_chunks)
    parsed = extract_answer_and_links(raw_output)
    return QueryResponse(answer=parsed["answer"], links=parsed["links"])

@app.get("/health")
def health():
    return {"status": "ok", "chunks": len(chunks), "embeddings": len(embeddings)}

@app.get("/")
def read_root():
    return {"message": "TDS Semantic Search App is running!"}

# Local dev run
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)
